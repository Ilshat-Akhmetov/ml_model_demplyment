# Sklearn модель на Flask в Docker контейнере.

Данная программа описывает пример того, как можно завернуть 
обученную модель во Flask, затем в Docker.

В данном случае ML-модель - игрушечная линейная регрессия двух переменных 
Y = a1*x1+a2*x2+b на искусственно сгенерированных данных x1,x2. Это объясняется тем, что проект
учебный. Однако при желании следуя примеру кода в данном мини-проекте 
легко можно заменить игрушечную модель 
на реальную. Файл с моделью - model_saved.pkl. Полный код генерации модели можно посмотреть
в jupyter-notebook файле API_requests_example.

В main.py размещен код реализации модели на Flask.

В requirements.txt - библиотеки, необходимые для работы модели в docker контейнере.

## Создание docker-контейнера для данной модели.

В ОС (например Ubuntu) с предустановленным docker перейдите в каталог с данным кодом 
и наберите

docker build -t test_flask .   (точка тоже нужна)

test_flask - имя для образа, который создаете. Можете выбрать сами.

В данном каталоге имеется файл Dockerfile, где определены все необходимые команды для создания образа image. Его
и считывает вышеприведенная команда при создании.

## Создание контейнера

После создания образа сначала нужно запустить контейнер. Пример того, как это можно сделать:

docker run  --rm --expose 80 -p 80:80 test_flask

Этой командой мы так же открываем порт 80 для того, чтобы запросы, приходящие хосту 
на некоторый ip с портом 80 пробрасывались контейнеру на тот же порт, затем приходил ответ.

## Взаимодействие с моделью

В данном проекте реализованы 3 способа взаимодействия с этим по сути клиент-серверным приложением.

Они описаны в ноутбуке API_requests_example.ipynb. 

Предположим, контейнер запущен. В терминале вы увидите что-то вроде 

 * Serving Flask app 'first_app' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
 * Running on http://172.17.0.2:80/ (Press CTRL+C to quit)

Следовательно, мы можем обращаться к контейнеру по адресу http://172.17.0.2:80/.

После запуска контейнера вы можете воспользоваться одним из трех способов, реализованных здесь:

1) http://172.17.0.2/prediction_var3 - вы перейдете на веб-форму, где вам будет предложено ввести значения x1, x2 и получить ответ
2) Для начала нужно сформировать файл в формате json: {'x1': x1_vale, 'x2':x2_value}. Затем отправить этот файл как post запрос на данный адрес http://172.17.0.2/prediction_var2 и получить ответ. Это можно сделать, например, с помощью библиотеки requests.
3) Перейти в браузере по адресу http://172.17.0.2/prediction_var1/<x1>/<x2>, введя вместо x1, x2 нужные вам значения, и получить ответ



